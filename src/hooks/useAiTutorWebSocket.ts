import { useCallback, useEffect, useRef, useState } from 'react';
import { aiTutorService, buildTutorWsUrl } from '../services/aiTutorService';

export interface TutorMessage {
  id?: string;
  role: 'user' | 'assistant' | 'system';
  content: string;
  createdAt?: string;
  files?: { preview: string; id: string; type: 'image' | 'pdf'; name?: string }[]; // File previews for display
}

type ConnectionStatus = 'connecting' | 'connected' | 'disconnected' | 'error';

interface UploadedFile {
  file: File;
  preview: string;
  id: string;
  type: 'image' | 'pdf';
}

interface SendOptions {
  isAudio?: boolean;
  token?: string;
  timezone?: string;
  languageCode?: string;
  audioUrl?: string;
  images?: UploadedFile[]; // Array of uploaded images
  pdfs?: UploadedFile[]; // Array of uploaded PDFs
  filePreviews?: { preview: string; id: string; type: 'image' | 'pdf'; name?: string }[]; // File previews for display
}

// Phoneme data structure for Unity
interface PhonemeData {
  time: number;        // milliseconds
  blendshape: string;  // Unity blendshape name
  weight: number;      // 0.0 to 1.0
}

export function useAiTutorWebSocket(userId: number) {
  const [messages, setMessages] = useState<TutorMessage[]>([]);
  const [isTyping, setIsTyping] = useState(false);
  const [thinkingMessage, setThinkingMessage] = useState<string>('');
  const [thinkingStatus, setThinkingStatus] = useState<string>('');
  const [isStreaming, setIsStreaming] = useState(false);
  const [connectionStatus, setConnectionStatus] = useState<ConnectionStatus>('disconnected');
  const [currentConversationId, setCurrentConversationId] = useState<string | null>(null);
  const [isConnected, setIsConnected] = useState(false);

  const wsRef = useRef<WebSocket | null>(null);
  const audioQueueRef = useRef<Array<{url: string, phonemes: PhonemeData[]}>>([]);
  const isPlayingAudioRef = useRef<boolean>(false);

  // Convert backend phonemes to Unity format (same as NextJS implementation)
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  const convertToUnityFormat = useCallback((backendPhonemes: any[]): PhonemeData[] => {
    if (!backendPhonemes || backendPhonemes.length === 0) return [];
    
    const mapBlendshape = (name?: string): string => {
      if (!name) return 'sil';
      const n = String(name);
      const table: Record<string, string> = {
        'AE': 'AE', 'IH': 'IH', 'EE': 'EE', 'Er': 'Er', 'IY': 'EE', 'E': 'EE',
        'AA': 'Ah', 'AH': 'Ah', 'A': 'Ah', 'AO': 'Oh', 'OW': 'Oh', 'OY': 'Oh', 'O': 'Oh',
        'UW': 'W_OO', 'UH': 'W_OO', 'U': 'W_OO', 'W_OO': 'W_OO',
        'OH': 'Oh', 'Oh': 'Oh', 'Ah': 'Ah', 'TH': 'TH',
        'T_L_D_N': 'T_L_D_N', 'K_G_H_NG': 'K_G_H_NG', 'B_M_P': 'B_M_P',
        'S_Z': 'S_Z', 'F_V': 'F_V', 'R': 'R',
        'L': 'T_L_D_N', 'D': 'T_L_D_N', 'N': 'T_L_D_N', 'T': 'T_L_D_N',
        'G': 'K_G_H_NG', 'H': 'K_G_H_NG', 'NG': 'K_G_H_NG', 'K': 'K_G_H_NG',
        'P': 'B_M_P', 'B': 'B_M_P', 'M': 'B_M_P',
        'Z': 'S_Z', 'S': 'S_Z', 'V': 'F_V', 'F': 'F_V',
        'Ch_J': 'Ch_J', 'sil': 'sil'
      };
      return table[n] || n;
    };

    const defaultWeight = (blendshape: string): number => {
      switch (blendshape) {
        case 'Ah': return 0.9;
        case 'Oh': return 0.85;
        case 'AE': return 0.8;
        case 'EE': return 0.6;
        case 'IH': return 0.5;
        case 'W_OO': return 0.6;
        case 'Er': return 0.5;
        case 'sil': return 0.0;
        default: return 0.35;
      }
    };

    const raw = backendPhonemes.map(phoneme => {
      const blend = mapBlendshape(phoneme.unity_blendshape || phoneme.phoneme);
      return {
        time: Math.round((phoneme.start_time || 0) * 1000), // Convert seconds to milliseconds
        blendshape: blend,
        weight: defaultWeight(blend)
      } as PhonemeData;
    });

    // Sort and collapse near-duplicates
    raw.sort((a, b) => a.time - b.time);
    const MIN_GAP_MS = 30;
    const smoothed: PhonemeData[] = [];
    for (const p of raw) {
      const last = smoothed[smoothed.length - 1];
      if (last && last.blendshape === p.blendshape && p.time - last.time < MIN_GAP_MS) continue;
      smoothed.push(p);
    }
    return smoothed;
  }, []);

  // âœ… NOTE: Audio queue management moved to Unity HTML
  // Unity maintains its own seamless queue and plays chunks continuously without gaps
  // Frontend just sends chunks as they arrive - Unity handles the rest

  const disconnect = useCallback(() => {
    if (wsRef.current) {
      try { wsRef.current.close(); } catch { /* ignore */ }
      wsRef.current = null;
    }
    // Clear audio queue
    audioQueueRef.current = [];
    isPlayingAudioRef.current = false;
    setIsConnected(false);
    setConnectionStatus('disconnected');
  }, []);

  const connect = useCallback((conversationId: string) => {
    const wsUrl = buildTutorWsUrl(conversationId);
    setConnectionStatus('connecting');
    try {
      const ws = new WebSocket(wsUrl);
      wsRef.current = ws;

      ws.onopen = () => {
        setIsConnected(true);
        setConnectionStatus('connected');
      };

      ws.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data);
          const messageType = data?.mt || data?.type; // Support both mt and type
          
          console.log('[AI Tutor] Received message:', messageType, data);
          
          switch (messageType) {
            case 'connected':
              console.log('[AI Tutor] Connected:', data.message);
              break;
              
            case 'user_message_received':
              // User message confirmation - already added when sent, don't add again
              console.log('[AI Tutor] User message confirmed:', data.message);
              // Don't add to messages - already added in sendMessage
              break;
              
            case 'stream_start':
              console.log('[AI Tutor] stream_start - Setting typing and thinking indicators');
              setIsStreaming(true);
              setIsTyping(true);
              // Always set a thinking status during streaming
              setThinkingStatus('generating');
              setThinkingMessage('Generating response...');
              // Initialize new message
              setMessages((prev) => [...prev, { 
                role: 'assistant', 
                content: '',
                id: data.messageId 
              }]);
              break;
              
            case 'stream_chunk':
              // Append chunk to the last message (assistant message)
              setIsTyping(true);
              setMessages((prev) => {
                const newMessages = [...prev];
                const lastMsg = newMessages[newMessages.length - 1];
                if (lastMsg && lastMsg.role === 'assistant') {
                  // Only append if chunk is not already in content (prevent duplication)
                  const chunk = data.chunk || '';
                  if (!lastMsg.content.endsWith(chunk)) {
                    lastMsg.content = (lastMsg.content || '') + chunk;
                  }
                }
                return newMessages;
              });
              break;
              
            case 'stream_end':
              setIsTyping(false);
              setIsStreaming(false);
              // Clear thinking indicators
              setThinkingMessage('');
              setThinkingStatus('');
              // Finalize the message with fullText (if provided and different)
              if (data.fullText) {
                setMessages((prev) => {
                  const newMessages = [...prev];
                  const lastMsg = newMessages[newMessages.length - 1];
                  if (lastMsg && lastMsg.role === 'assistant') {
                    // Only replace if fullText is different (prevents unnecessary re-render)
                    if (lastMsg.content !== data.fullText) {
                      lastMsg.content = data.fullText;
                    }
                  }
                  return newMessages;
                });
              }
              break;
              
            case 'typing_indicator':
              setIsTyping(true);
              break;
              
            case 'stop_typing_indicator':
              setIsTyping(false);
              break;
              
            case 'thinking_indicator':
              // Backend sends thinking status like "Analyzing...", "Searching...", etc.
              console.log('[AI Tutor] âœ… Thinking indicator received:', data.message, data.status);
              setIsTyping(true);
              setThinkingMessage(data.message || '');
              setThinkingStatus(data.status || 'thinking');
              console.log('[AI Tutor] Set state - isTyping: true, message:', data.message, 'status:', data.status);
              break;
              
            case 'error':
              console.error('[AI Tutor] Error:', data.error);
              setIsTyping(false);
              setIsStreaming(false);
              break;
              
            // Legacy support
            case 'user_message':
              // Only add if not already in messages (prevent duplicates from backend echo)
              if (data?.message) {
                setMessages((prev) => {
                  const lastMessage = prev[prev.length - 1];
                  const messageText = String(data.message);
                  // Don't add if last message is the same (prevents duplicate)
                  if (lastMessage && lastMessage.role === 'user' && lastMessage.content === messageText) {
                    console.log('[AI Tutor] Duplicate user_message from backend ignored');
                    return prev;
                  }
                  return [...prev, { role: 'user', content: messageText }];
                });
              }
              break;
              
            case 'bot_message':
              if (data?.message) {
            setMessages((prev) => [...prev, { role: 'assistant', content: String(data.message) }]);
              }
              break;
              
            case 'status':
            setThinkingStatus(String(data.status || ''));
            setThinkingMessage(String(data.message || ''));
              break;
              
            case 'audio':
            case 'audio_chunk': {
              // Handle audio chunk with phonemes for Unity avatar
              console.log('[AI Tutor] ðŸŽµ ==================== AUDIO RECEIVED ====================');
              console.log('[AI Tutor] ðŸŽµ Raw backend data:', {
                audioUrl: data?.audioUrl,
                phonemeCount: data?.phonemes?.length || 0,
                text: data?.text,
                firstPhoneme: data?.phonemes?.[0]
              });
              
              // Convert backend phonemes to Unity format (user's Unity expects simple format)
              const backendPhonemes = data?.phonemes || [];
              const unityPhonemes = convertToUnityFormat(backendPhonemes);
              
              console.log('[AI Tutor] ðŸŽ­ Backend phonemes:', backendPhonemes.length);
              console.log('[AI Tutor] ðŸŽ­ Unity phonemes:', unityPhonemes.length);
              if (unityPhonemes.length > 0) {
                console.log('[AI Tutor] ðŸŽ­ First Unity phoneme:', unityPhonemes[0]);
                console.log('[AI Tutor] ðŸŽ­ Last Unity phoneme:', unityPhonemes[unityPhonemes.length - 1]);
              }
              
              // Prepare audio URL - Use backend URL for audio files
              let audioUrl = null;
              if (data?.audioUrl) {
                if (data.audioUrl.startsWith('http')) {
                  audioUrl = data.audioUrl;
                } else {
                  const backendUrl = import.meta.env.VITE_API_BASE_URL || import.meta.env.VITE_API_URL || 'http://localhost:5000';
                  try {
                    const urlObj = new URL(backendUrl);
                    audioUrl = `${urlObj.protocol}//${urlObj.host}${data.audioUrl}`;
                    console.log('[AI Tutor] ðŸŽµ Audio URL (backend):', audioUrl);
                  } catch (e) {
                    const host = window.location.hostname;
                    audioUrl = `http://${host}:5000${data.audioUrl}`;
                    console.log('[AI Tutor] ðŸŽµ Audio URL (fallback):', audioUrl);
                  }
                }
              }
              
              // âœ… Send immediately to Unity for seamless continuous playback
              // Unity maintains its own internal queue and plays chunks one after another without gaps
              if (unityPhonemes.length > 0 && audioUrl) {
                console.log('[AI Tutor] ðŸŽµ Sending audio chunk to Unity (seamless continuous playback)');
                
                // âœ… Send directly to Unity - it handles queue and seamless playback
                const iframes = document.querySelectorAll('iframe[src*="/WebGL/index.html"]') as NodeListOf<HTMLIFrameElement>;
                iframes.forEach((iframe) => {
                  if (iframe?.contentWindow) {
                    iframe.contentWindow.postMessage({
                      type: 'PLAY_TTS_WITH_PHONEMES',
                      payload: {
                        audioUrl: audioUrl,
                        phonemes: unityPhonemes,
                        id: `audio-${Date.now()}`
                      }
                    }, '*');
                    console.log('[AI Tutor] âœ… Sent to Unity seamless queue:', {
                      audioUrl: audioUrl,
                      phonemesCount: unityPhonemes.length
                    });
                  }
                });
              } else {
                console.warn('[AI Tutor] âš ï¸ Skipping audio - no phonemes or URL');
            }
              break;
            }
              
            case 'audio_generation_start':
              console.log('[AI Tutor] Audio generation started');
              break;
              
            case 'audio_generation_complete':
              console.log('[AI Tutor] Audio generation complete');
              break;
              
            default:
              // Fallback: if it's a string, treat as assistant message
              if (typeof data === 'string') {
            setMessages((prev) => [...prev, { role: 'assistant', content: data }]);
              } else if (data?.message && !messageType) {
                // Unknown format but has message field
                setMessages((prev) => [...prev, { role: 'assistant', content: String(data.message) }]);
              }
          }
        } catch (e) {
          console.error('[AI Tutor] Error parsing message:', e);
          // Non-JSON fallbacks
          if (typeof event.data === 'string') {
            setMessages((prev) => [...prev, { role: 'assistant', content: event.data }]);
          }
        }
      };

      ws.onerror = () => {
        setConnectionStatus('error');
        setIsConnected(false);
      };

      ws.onclose = () => {
        setIsConnected(false);
        setConnectionStatus('disconnected');
      };
    } catch (e) {
      console.error('[AI Tutor] Connection error:', e);
      setConnectionStatus('error');
      setIsConnected(false);
    }
  }, [convertToUnityFormat]);

  const createNewConversation = useCallback(async (subject: string, topic: string) => {  // âœ… Add topic parameter
    const res = await aiTutorService.createConversation({ subject, topic, user_id: userId });  // âœ… Pass topic
    if (res?.success && res?.data?.conversation_id) {
      const id = res.data.conversation_id;
      setCurrentConversationId(id);
      // Reset state for fresh session
      setMessages([]);
      setThinkingMessage('');
      setThinkingStatus('');
      disconnect();
      connect(id);
      return id;
    }
    throw new Error('Failed to create conversation');
  }, [connect, disconnect, userId]);

  const endConversation = useCallback(async () => {
    if (!currentConversationId) return;
    await aiTutorService.endConversation(currentConversationId);
    disconnect();
  }, [currentConversationId, disconnect]);

  const sendMessage = useCallback(async (text: string, opts?: SendOptions) => {
    // Prevent duplicate messages - check if last message is the same
    const trimmedText = text.trim();
    const hasFiles = (opts?.images && opts.images.length > 0) || (opts?.pdfs && opts.pdfs.length > 0);
    if (trimmedText === '' && !hasFiles) return;
    
    // Check connection status with better logging
    if (!currentConversationId) {
      console.error('[AI Tutor] Cannot send message - no conversation ID');
      return;
    }
    
    if (!wsRef.current) {
      console.error('[AI Tutor] Cannot send message - WebSocket not initialized');
      return;
    }
    
    if (wsRef.current.readyState !== WebSocket.OPEN) {
      console.error('[AI Tutor] Cannot send message - WebSocket not open. State:', wsRef.current.readyState);
      return;
    }
    
    let imageUrls: string[] = [];
    let pdfUrls: string[] = [];
    
    // Upload images if present
    if (opts?.images && opts.images.length > 0) {
      console.log(`[AI Tutor] ðŸ–¼ï¸ Starting image upload: ${opts.images.length} image(s)`);
      try {
        const formData = new FormData();
        opts.images.forEach((img, idx) => {
          console.log(`[AI Tutor] ðŸ“Ž Adding image ${idx + 1}: ${img.file.name} (${img.file.size} bytes)`);
          formData.append('images', img.file);
        });
        formData.append('conversation_id', currentConversationId);
        
        const apiBaseUrl = import.meta.env.VITE_API_BASE_URL || import.meta.env.VITE_API_URL || 'http://localhost:5000';
        console.log('[AI Tutor] ðŸ“¤ Uploading images to:', `${apiBaseUrl}/ai-tutor/upload-images`);
        const response = await fetch(`${apiBaseUrl}/ai-tutor/upload-images`, {
          method: 'POST',
          body: formData,
        });
        
        console.log('[AI Tutor] ðŸ“¥ Upload response status:', response.status);
        if (response.ok) {
          const data = await response.json();
          imageUrls = data.image_urls || [];
          console.log('[AI Tutor] âœ… Images uploaded successfully!');
          console.log('[AI Tutor] ðŸ–¼ï¸ Image URLs:', imageUrls);
        } else {
          const errorText = await response.text();
          console.error('[AI Tutor] âŒ Image upload failed:', response.status, errorText);
        }
      } catch (error) {
        console.error('[AI Tutor] ðŸ’¥ Error uploading images:', error);
      }
    }
    
    // Upload PDFs if present
    if (opts?.pdfs && opts.pdfs.length > 0) {
      console.log(`[AI Tutor] ðŸ“„ Starting PDF upload: ${opts.pdfs.length} PDF(s)`);
      try {
        const formData = new FormData();
        opts.pdfs.forEach((pdf, idx) => {
          console.log(`[AI Tutor] ðŸ“Ž Adding PDF ${idx + 1}: ${pdf.file.name} (${pdf.file.size} bytes)`);
          formData.append('pdfs', pdf.file);
        });
        formData.append('conversation_id', currentConversationId);
        
        const apiBaseUrl = import.meta.env.VITE_API_BASE_URL || import.meta.env.VITE_API_URL || 'http://localhost:5000';
        console.log('[AI Tutor] ðŸ“¤ Uploading PDFs to:', `${apiBaseUrl}/ai-tutor/upload-pdfs`);
        const response = await fetch(`${apiBaseUrl}/ai-tutor/upload-pdfs`, {
          method: 'POST',
          body: formData,
        });
        
        console.log('[AI Tutor] ðŸ“¥ PDF upload response status:', response.status);
        if (response.ok) {
          const data = await response.json();
          pdfUrls = data.pdf_urls || [];
          console.log('[AI Tutor] âœ… PDFs uploaded successfully!');
          console.log('[AI Tutor] ðŸ“„ PDF URLs:', pdfUrls);
        } else {
          const errorText = await response.text();
          console.error('[AI Tutor] âŒ PDF upload failed:', response.status, errorText);
        }
      } catch (error) {
        console.error('[AI Tutor] ðŸ’¥ Error uploading PDFs:', error);
      }
    }
    
    setIsTyping(true);
    
    // Prepare default message based on content
    let defaultMessage = 'Please analyze this content';
    if (imageUrls.length > 0 && pdfUrls.length > 0) {
      defaultMessage = 'Please analyze these images and documents';
    } else if (imageUrls.length > 0) {
      defaultMessage = 'Please analyze these images';
    } else if (pdfUrls.length > 0) {
      defaultMessage = 'Please analyze this document';
    }
    
    // Match nextjscode format - use 'mt' instead of 'type', 'is_audio' instead of 'isAudio'
    const payload = {
      mt: "message_upload",
      message: trimmedText || defaultMessage,
      content: trimmedText || defaultMessage, // Also include content field for compatibility
      timestamp: new Date().toISOString(),
      userId: userId.toString(), // Ensure userId is string
      timezone: opts?.timezone || Intl.DateTimeFormat().resolvedOptions().timeZone,
      languageCode: opts?.languageCode || 'en',
      is_audio: opts?.isAudio ?? false, // Use is_audio (backend supports both but prefers is_audio)
      isAudio: opts?.isAudio ?? false, // Also include isAudio for backward compatibility
      token: opts?.token || `msg_${Date.now()}`,
      audio_url: opts?.audioUrl || null, // Support audio_url if provided
      images: imageUrls.length > 0 ? imageUrls : undefined, // Add image URLs
      pdfs: pdfUrls.length > 0 ? pdfUrls : undefined, // Add PDF URLs
    };
    console.log('[AI Tutor] ðŸ“¨ Sending message to backend:');
    console.log('[AI Tutor] ðŸ“ Message:', payload.message);
    console.log('[AI Tutor] ðŸ–¼ï¸ Images:', payload.images);
    console.log('[AI Tutor] ðŸ“„ PDFs:', payload.pdfs);
    console.log('[AI Tutor] ðŸ“¦ Full payload:', payload);
    wsRef.current.send(JSON.stringify(payload));
    
    // Add message to local state only if not duplicate
    const displayContent = (imageUrls.length > 0 || pdfUrls.length > 0)
      ? `${trimmedText || defaultMessage}`
      : trimmedText;
    
    console.log('[AI Tutor] ðŸ’¬ Adding message to chat:');
    console.log('[AI Tutor] ðŸ“ Content:', displayContent);
    console.log('[AI Tutor] ðŸ“ File previews from opts:', opts?.filePreviews);
    console.log('[AI Tutor] ðŸ“¦ Files to add:', opts?.filePreviews?.length || 0);
    
    setMessages((prev) => {
      const lastMessage = prev[prev.length - 1];
      if (lastMessage && lastMessage.role === 'user' && lastMessage.content === displayContent) {
        console.log('[AI Tutor] Duplicate message prevented:', trimmedText);
        return prev; // Don't add duplicate
      }
      
      const newMessage = { 
        role: 'user' as const, 
        content: displayContent,
        files: opts?.filePreviews  // Add file previews to message
      };
      
      console.log('[AI Tutor] âœ… New message object:', newMessage);
      console.log('[AI Tutor] ðŸ–¼ï¸ Files in message:', newMessage.files);
      
      return [...prev, newMessage];
    });
    
    setTimeout(() => setIsTyping(false), 200); // lightweight UX
  }, [currentConversationId, userId]);

  // Cleanup on unmount
  useEffect(() => () => disconnect(), [disconnect]);

  return {
    messages,
    isTyping,
    isStreaming,
    thinkingMessage,
    thinkingStatus,
    isConnected,
    connectionStatus,
    currentConversationId,
    createNewConversation,
    endConversation,
    sendMessage,
  } as const;
}



